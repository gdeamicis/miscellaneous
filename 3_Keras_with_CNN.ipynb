{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3_Keras_with_CNN.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"view-in-github","colab_type":"text"},"cell_type":"markdown","source":["<a href=\"https://colab.research.google.com/github/youssefhedhili/miscellaneous/blob/master/3_Keras_with_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"metadata":{"id":"Sb_IxUlej-rC","colab_type":"text"},"cell_type":"markdown","source":["#Neural Networks with Python"]},{"metadata":{"id":"ejImbvsLnPFo","colab_type":"text"},"cell_type":"markdown","source":["**Importing Libraries and modules**"]},{"metadata":{"id":"60TX-rYBkMk6","colab_type":"text"},"cell_type":"markdown","source":["We will be using a library called Keras. It has all the functionality needed to construct, train and evaluate machine learning models. We will construct a feed-forward CNN, it is a basic type of neural network that forwards the information layer after layer.\n","\n","We will start importing the Sequential model from Keras. It is just a sequence of neural network layers, and is perfect for constructing such a network."]},{"metadata":{"id":"KSqSUM6ymFGS","colab_type":"code","outputId":"33f05e60-4f52-467c-c8dd-a0d472392c87","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1554452464459,"user_tz":-120,"elapsed":2015,"user":{"displayName":"Gonzalo De Amicis","photoUrl":"https://lh5.googleusercontent.com/-aVtB1Sqieps/AAAAAAAAAAI/AAAAAAAAAI8/KNhk7ccWoSA/s64/photo.jpg","userId":"09824824751340945613"}}},"cell_type":"code","source":["# Keras is already installed on Colab, so we only need to import it\n","from keras.models import Sequential"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"BLAp0UV6lxbZ","colab_type":"text"},"cell_type":"markdown","source":["Next, let's import the \"core\" layers from Keras. These are the layers that are used in almost any neural network."]},{"metadata":{"id":"l9Rwj5ebmfoY","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras.layers import Dense, Dropout, Activation, Flatten"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tReEnapCl2xW","colab_type":"text"},"cell_type":"markdown","source":["Then, we'll import the CNN layers from Keras. These are the convolutional layers that will help us efficiently train on image data."]},{"metadata":{"id":"RK3CQjVpnD6Y","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras.layers import Convolution2D, MaxPooling2D"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CfAxmVtvl63G","colab_type":"text"},"cell_type":"markdown","source":["Finally, we'll import some utilities. This will help us transform our data later."]},{"metadata":{"id":"Co9pI-hQnGMa","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras.utils import np_utils"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ir6_SohHnuBI","colab_type":"text"},"cell_type":"markdown","source":["**Load image data from MNIST**"]},{"metadata":{"id":"AAKr4a1Kl-Fd","colab_type":"text"},"cell_type":"markdown","source":["MNIST is a dataset that consist on 60000 images of hand-written digits in a size of 28x28 pixels.\n","\n","It is a great dataset for getting started with deep learning and computer vision. It's a big enough challenge to neural networks, but it's manageable on a single computer.\n","\n","\n","We will importing it from the Keras package, and separating it into 2 sets: train set and test set.\n","For each set, we will have a set of images, and a set of the corresponding labels of the images."]},{"metadata":{"id":"m-9bJrCOnKcd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"1d452ad6-2eff-4d33-8c41-aff41d5d4986","executionInfo":{"status":"ok","timestamp":1554453139072,"user_tz":-120,"elapsed":2211,"user":{"displayName":"Gonzalo De Amicis","photoUrl":"https://lh5.googleusercontent.com/-aVtB1Sqieps/AAAAAAAAAAI/AAAAAAAAAI8/KNhk7ccWoSA/s64/photo.jpg","userId":"09824824751340945613"}}},"cell_type":"code","source":["from keras.datasets import mnist\n"," \n","# Load MNIST data into train and test sets\n","(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n","11493376/11490434 [==============================] - 1s 0us/step\n"],"name":"stdout"}]},{"metadata":{"id":"d7R4rYeVniQ_","colab_type":"text"},"cell_type":"markdown","source":["Let's look at the shape of the images train dataset"]},{"metadata":{"id":"G7bbmGUFnshV","colab_type":"code","outputId":"a309ac42-44b3-4ec1-9620-743ad81ca3dc","colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["print(train_images.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(60000, 28, 28)\n"],"name":"stdout"}]},{"metadata":{"id":"RFgn_KoDn4SH","colab_type":"text"},"cell_type":"markdown","source":["Great, so it appears that we have 60,000 samples in our training set, and the images are 28 pixels x 28 pixels each. We can confirm this by plotting the first sample in matplotlib."]},{"metadata":{"id":"GFyhJnJYoAf_","colab_type":"code","outputId":"e5ba01c5-b098-4ad8-cb0c-cde305860a43","colab":{"base_uri":"https://localhost:8080/","height":364},"executionInfo":{"status":"ok","timestamp":1554453185376,"user_tz":-120,"elapsed":791,"user":{"displayName":"Gonzalo De Amicis","photoUrl":"https://lh5.googleusercontent.com/-aVtB1Sqieps/AAAAAAAAAAI/AAAAAAAAAI8/KNhk7ccWoSA/s64/photo.jpg","userId":"09824824751340945613"}}},"cell_type":"code","source":["from matplotlib import pyplot as plt\n","plt.imshow(train_images[0])\n","print('Expected Label: ', train_labels[0])"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Expected Label:  5\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 576x396 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"K3Tl5a5OFQAQ","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras import backend as K\n","K.set_image_dim_ordering('th')\n","# input image dimensions\n","img_rows, img_cols = 28, 28\n","\n","\n","if K.image_data_format() == 'channels_first':\n","    train_images = train_images.reshape(train_images.shape[0], 1, img_rows, img_cols)\n","    test_images = test_images.reshape(test_images.shape[0], 1, img_rows, img_cols)\n","    input_shape = (1, img_rows, img_cols)\n","else:\n","    train_images = train_images.reshape(train_images.shape[0], img_rows, img_cols, 1)\n","    test_images = test_images.reshape(test_images.shape[0], img_rows, img_cols, 1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vtM_yZZFoD6b","colab_type":"code","outputId":"f74853b0-f05a-474f-9c77-6fdee7ef5f44","colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["#When using the Theano backend, you must explicitly declare a dimension for the depth of the input image. For example, a full-color image with all 3 RGB channels will have a depth of 3.\n","#Our MNIST images only have a depth of 1, but we must explicitly declare that.\n","#In other words, we want to transform our dataset from having shape (n, width, height) to (n, depth, width, height).\n","#Here's how we can do that easily:\n","\n","#Convert data type and normalize values\n","\n","train_images = train_images.astype('float32')\n","test_images = test_images.astype('float32')\n","train_images /= 255\n","test_images /= 255\n","\n","print(train_images.shape)\n","\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(60000, 1, 28, 28)\n"],"name":"stdout"}]},{"metadata":{"id":"8sVHHeSvoYGN","colab_type":"text"},"cell_type":"markdown","source":["**Preprocess class labels for Keras**"]},{"metadata":{"id":"XZdAzCcno3qR","colab_type":"text"},"cell_type":"markdown","source":["Now, to have a proper training we need the labels to be an zero array, with a one on the expected label:\n","\n","For example, for a label expected of 5:\n","[0, 0, 0, 0, 0, 1, 0, 0, 0, 0 ]\n","\n","Let's take a look at the shape of our class label data.\n"]},{"metadata":{"id":"hrjAeP4moWIt","colab_type":"code","outputId":"dcba7b14-e003-4c38-ea65-d5f98591f8bd","colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["print(train_labels.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(60000,)\n"],"name":"stdout"}]},{"metadata":{"id":"6YdDtoZBpa4h","colab_type":"text"},"cell_type":"markdown","source":["We should have 10 different classes, one for each digit, but we have only a 1-dimensional array.\n","\n","Let's take a look at the labels for the first 10 images."]},{"metadata":{"id":"8H_X3gcuoug4","colab_type":"code","outputId":"404d5577-bb69-4f4a-f336-ce472c970bd0","colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["print(train_labels[:10])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[5 0 4 1 9 2 1 3 1 4]\n"],"name":"stdout"}]},{"metadata":{"id":"hr6ut50Epu5K","colab_type":"text"},"cell_type":"markdown","source":["So the problem is that the labels are not split into 10 class labels, but represented as a single array with the class values.\n","\n","We will fix it converting the 1-dimensional arrays to 10-dimensional class matrices"]},{"metadata":{"id":"ednZpjfeo_LE","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras import backend as K\n","K.set_image_dim_ordering('th')\n","\n","train_labels = np_utils.to_categorical(train_labels, 10)\n","test_labels = np_utils.to_categorical(test_labels, 10)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"W26DnP6gqLTe","colab_type":"text"},"cell_type":"markdown","source":["Now let's take a look"]},{"metadata":{"id":"kvthP9wvo157","colab_type":"code","outputId":"0a1142d1-344e-4951-9b27-fbf7d8d780b2","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1554453707075,"user_tz":-120,"elapsed":1027,"user":{"displayName":"Gonzalo De Amicis","photoUrl":"https://lh5.googleusercontent.com/-aVtB1Sqieps/AAAAAAAAAAI/AAAAAAAAAI8/KNhk7ccWoSA/s64/photo.jpg","userId":"09824824751340945613"}}},"cell_type":"code","source":["print(train_labels.shape)\n","\n","print(train_labels[:10])"],"execution_count":14,"outputs":[{"output_type":"stream","text":["(60000, 10)\n","[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"],"name":"stdout"}]},{"metadata":{"id":"6aEXUCxbpM3c","colab_type":"text"},"cell_type":"markdown","source":["**Define model architecture**"]},{"metadata":{"id":"P11HivTEqWVt","colab_type":"text"},"cell_type":"markdown","source":["Now we are ready to define the architecture of the model.\n","\n","First we will declare a sequential model format."]},{"metadata":{"id":"Lthy1F5PpOhv","colab_type":"code","colab":{}},"cell_type":"code","source":["model = Sequential()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kj1M2CD5qiIP","colab_type":"text"},"cell_type":"markdown","source":["Next, we declare the input layer"]},{"metadata":{"id":"tZKj6dTPpZ90","colab_type":"code","outputId":"41daa315-ab79-4df3-81f9-6fda61b34d38","colab":{"base_uri":"https://localhost:8080/","height":122}},"cell_type":"code","source":["#CNN input layer\n","\n","model.add(Convolution2D(32, 3, 3, activation='relu', input_shape=(1,28,28)))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", input_shape=(1, 28, 28...)`\n","  \n"],"name":"stderr"}]},{"metadata":{"id":"RhhdhR75qvEy","colab_type":"text"},"cell_type":"markdown","source":["The input shape parameter should be the shape of 1 sample. In this case, it's the same (1, 28, 28) that corresponds to  the (depth, width, height) of each image.\n","\n","The first 3 parameters correspond to the number of convolution filters to use, the number of rows in each convolution kernel, and the number of columns in each convolution kernel, respectively. So we have 32 convolution filters, each as a 3x3 matrix.\n","\n","We can confirm this by printing the shape of the current model output."]},{"metadata":{"id":"9tuO08BDpfc-","colab_type":"code","outputId":"3c0ccd13-eeba-4fbe-99a1-2e93bc2a964b","colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["print(model.output_shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(None, 32, 26, 26)\n"],"name":"stdout"}]},{"metadata":{"id":"LZRWsHUarGNh","colab_type":"text"},"cell_type":"markdown","source":["Next, we can simply add more layers to our model like we're building legos"]},{"metadata":{"id":"PJywO6KjpvFP","colab_type":"code","outputId":"2cdd80bd-8487-4e3b-a007-06998aaa7fb9","colab":{"base_uri":"https://localhost:8080/","height":122}},"cell_type":"code","source":["model.add(Convolution2D(32, 3, 3, activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2,2)))\n","model.add(Dropout(0.25))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\")`\n","  \n"],"name":"stderr"}]},{"metadata":{"id":"vCJJcGCorKNp","colab_type":"text"},"cell_type":"markdown","source":["We won't go into the theory too much, but it's important to highlight the Dropout layer we just added. This is a method for regularizing our model in order to prevent overfitting.\n","\n","MaxPooling2D is a way to reduce the number of parameters in our model by sliding a 2x2 pooling filter across the previous layer and taking the max of the 4 values in the 2x2 filter.\n","\n","So far, for model parameters, we've added two Convolution layers. To complete our model architecture, let's add a fully connected layer and then the output layer.\n"]},{"metadata":{"id":"pn5aCWBrqKCm","colab_type":"code","colab":{}},"cell_type":"code","source":["#Fully connected Dense layers\n","\n","model.add(Flatten())\n","model.add(Dense(128, activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(10, activation='softmax'))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"oXIztBFqsIDm","colab_type":"text"},"cell_type":"markdown","source":["For Dense layers, the first parameter is the output size of the layer. Keras automatically handles the connections between layers.\n","\n","Note that the final layer has an output size of 10, corresponding to the 10 classes of digits.\n","\n","Also note that the weights from the Convolution layers must be flattened (made 1-dimensional) before passing them to the fully connected Dense layer."]},{"metadata":{"id":"AIgnb0l_sNEZ","colab_type":"text"},"cell_type":"markdown","source":["Finally the architecture of the model will be:\n","\n"]},{"metadata":{"id":"qfZtdShd7Nwj","colab_type":"code","colab":{}},"cell_type":"code","source":["#Model architecture\n","# model = Sequential()\n"," \n","# model.add(Convolution2D(32, 3, 3, activation='relu', input_shape=(1,28,28)))\n","# model.add(Convolution2D(32, 3, 3, activation='relu'))\n","# model.add(MaxPooling2D(pool_size=(2,2)))\n","# model.add(Dropout(0.25))\n"," \n","# model.add(Flatten())\n","# model.add(Dense(128, activation='relu'))\n","# model.add(Dropout(0.5))\n","# model.add(Dense(10, activation='softmax'))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HnztiPpq7Y5W","colab_type":"text"},"cell_type":"markdown","source":["**Compile the model**"]},{"metadata":{"id":"u1KnTz7_ro6p","colab_type":"text"},"cell_type":"markdown","source":["When we compile the model, we declare the loss function and the optimizer (SGD, Adam, etc.)"]},{"metadata":{"id":"jpCLJYVM7as_","colab_type":"code","colab":{}},"cell_type":"code","source":["model.compile(loss='categorical_crossentropy',\n","              optimizer='adam',\n","              metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IqDyUme77fYa","colab_type":"text"},"cell_type":"markdown","source":["**Fit model on training data**"]},{"metadata":{"id":"j0nFmy3JrsYl","colab_type":"text"},"cell_type":"markdown","source":["To fit the model, all we have to do is declare the batch size and number of epochs to train for, then pass in our training data."]},{"metadata":{"id":"8NCORpct7e3y","colab_type":"code","outputId":"1bcb2851-136e-4225-b52e-fd74add961bf","colab":{"base_uri":"https://localhost:8080/","height":479}},"cell_type":"code","source":["#Fit Keras model\n","model.fit(train_images, train_labels, \n","          batch_size=32, nb_epoch=10, verbose=1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n","  \n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/10\n","60000/60000 [==============================] - 18s 301us/step - loss: 0.2048 - acc: 0.9368\n","Epoch 2/10\n","60000/60000 [==============================] - 17s 280us/step - loss: 0.0865 - acc: 0.9740\n","Epoch 3/10\n","60000/60000 [==============================] - 17s 280us/step - loss: 0.0659 - acc: 0.9805\n","Epoch 4/10\n","60000/60000 [==============================] - 17s 280us/step - loss: 0.0557 - acc: 0.9834\n","Epoch 5/10\n","60000/60000 [==============================] - 17s 280us/step - loss: 0.0452 - acc: 0.9860\n","Epoch 6/10\n","60000/60000 [==============================] - 17s 280us/step - loss: 0.0425 - acc: 0.9868\n","Epoch 7/10\n","60000/60000 [==============================] - 17s 280us/step - loss: 0.0359 - acc: 0.9884\n","Epoch 8/10\n","60000/60000 [==============================] - 17s 279us/step - loss: 0.0331 - acc: 0.9893\n","Epoch 9/10\n","60000/60000 [==============================] - 17s 279us/step - loss: 0.0315 - acc: 0.9902\n","Epoch 10/10\n","60000/60000 [==============================] - 17s 280us/step - loss: 0.0277 - acc: 0.9910\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f0f0dfdae10>"]},"metadata":{"tags":[]},"execution_count":22}]},{"metadata":{"id":"kYEBs6Gw7xdS","colab_type":"text"},"cell_type":"markdown","source":["**Evaluate model on test data**"]},{"metadata":{"id":"Me39XlOqsabo","colab_type":"text"},"cell_type":"markdown","source":["Finally, we can evaluate our model on the test data."]},{"metadata":{"id":"sp7KXBDB7ya5","colab_type":"code","outputId":"67a74d59-bd45-4c68-9c75-4fcfaa389a08","colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["score = model.evaluate(test_images, test_labels, verbose=0)\n","print(score)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[0.027494018173711266, 0.9921]\n"],"name":"stdout"}]}]}